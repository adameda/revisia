# app/llm.py

import os
import json
import random
from typing import List
from pydantic import BaseModel, Field
from dotenv import load_dotenv
from google import genai
from google.genai import types 

# Charger les variables d'environnement
load_dotenv()

# Initialiser le client Gemini
client = genai.Client(api_key=os.getenv("GEMINI_API_KEY"))

MODEL_NAME = "gemini-2.5-flash"

# Mode mock pour le d√©veloppement
MOCK_MODE = os.getenv("MOCK_GEMINI", "False").lower() == "true"


# --- Mod√®le de sortie attendu ---
class QuizItem(BaseModel):
    type: str = Field(description="Type de question, ici toujours 'qcm'.")
    question: str = Field(description="Intitul√© de la question.")
    choices: List[str] = Field(description="Liste des 4 choix possibles.")
    answer: str = Field(description="R√©ponse correcte.")

class QuizResponse(BaseModel):
    items: List[QuizItem]


# --- Template du prompt ---
PROMPT_TEMPLATE = """
**INSTRUCTIONS :**

Tu es un g√©n√©rateur de quiz p√©dagogique expert. Ta mission est de cr√©er un ensemble de questions √† choix multiples (QCM) bas√©es **uniquement** sur le cours fourni ci-dessous.

**R√®gle absolue :**
- N‚Äôutilise **aucune connaissance externe**.
- Chaque question et sa r√©ponse correcte doivent √™tre **directement justifiables** par le cours donn√©.

**T√ÇCHES :**
1. G√©n√®re **{nb_questions} questions QCM** pour enrichir une base de donn√©es.
2. Le jeu de questions doit **couvrir TOUT le cours** de mani√®re √©quilibr√©e : d√©finitions, concepts, noms, dates.
3. Chaque question doit :
   - √™tre courte et claire ;
   - avoir **4 choix plausibles** ;
   - contenir **une seule r√©ponse correcte** ;
   - √©viter les formulations ambigu√´s ou √©videntes.
4. Les r√©ponses doivent √™tre pr√©cises.

COURS √Ä ANALYSER :
<<<
{texte}
<<<
"""


def generate_mock_quiz(text: str, total_questions: int = 20) -> List[dict]:
    """
    G√©n√®re un quiz mock√© pour le d√©veloppement.
    √âvite les appels API co√ªteux pendant les tests.
    """
    print(f"üß™ MODE MOCK ACTIV√â - G√©n√©ration de {total_questions} questions fictives")
    
    # Extraire quelques mots cl√©s du texte pour rendre les questions plus r√©alistes
    words = text.split()[:50]  # Premiers 50 mots
    sample_words = random.sample([w for w in words if len(w) > 4], min(10, len([w for w in words if len(w) > 4])))
    
    questions = []
    question_types = [
        "Quelle est la d√©finition de {word} ?",
        "Parmi ces propositions, laquelle concerne {word} ?",
        "Quel concept est li√© √† {word} ?",
        "Comment peut-on d√©crire {word} ?",
        "Quelle affirmation est vraie concernant {word} ?",
    ]
    
    for i in range(total_questions):
        word = sample_words[i % len(sample_words)] if sample_words else f"concept_{i+1}"
        question_template = random.choice(question_types)
        
        correct_answer = f"R√©ponse correcte sur {word}"
        wrong_answers = [
            f"Fausse r√©ponse A sur {word}",
            f"Fausse r√©ponse B sur {word}",
            f"Fausse r√©ponse C sur {word}",
        ]
        
        # M√©langer les choix
        all_choices = [correct_answer] + wrong_answers
        random.shuffle(all_choices)
        
        questions.append({
            "type": "qcm",
            "question": question_template.format(word=word),
            "choices": all_choices,
            "answer": correct_answer,
            "explanation": f"Explication mock√©e pour la question {i+1}"
        })
    
    return questions


def generate_quiz_from_text(text: str, total_questions: int = 20):
    """
    Appelle le mod√®le Gemini pour g√©n√©rer un quiz structur√©.
    En mode MOCK, g√©n√®re des questions fictives.
    Retourne une liste de dictionnaires (items).
    """
    # Mode mock activ√©
    if MOCK_MODE:
        return generate_mock_quiz(text, total_questions)
    
    # Mode production - Appel r√©el √† Gemini
    print(f"ü§ñ Appel API Gemini - G√©n√©ration de {total_questions} questions r√©elles")
    prompt = PROMPT_TEMPLATE.format(texte=text, nb_questions=total_questions)

    response = client.models.generate_content(
        model=MODEL_NAME,
        contents=prompt,
        config=types.GenerateContentConfig(
            temperature=0.3,
            response_mime_type="application/json",
            response_json_schema=QuizResponse.model_json_schema()
        )
    )

    try:
        # Validation Pydantic automatique
        quiz = QuizResponse.model_validate_json(response.text)
        return [item.dict() for item in quiz.items]

    except Exception as e:
        print("‚ùå Erreur de parsing JSON :", e)
        print("R√©ponse brute :", response.text[:300])
        return []