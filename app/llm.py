# app/llm.py

import os
import json
import random
import logging
from typing import List, Tuple, Optional
from pydantic import BaseModel, Field
from dotenv import load_dotenv
from google import genai
from google.genai import types

# Charger les variables d'environnement
load_dotenv()

MODEL_NAME = "gemini-2.5-flash"

# Mode mock pour le d√©veloppement
MOCK_MODE = os.getenv("MOCK_GEMINI", "False").lower() == "true"

# Cl√©s API avec fallback
API_KEYS = [k for k in [
    os.getenv("GEMINI_API_KEY"),
    os.getenv("GEMINI_API_KEY_2"),
] if k]

logger = logging.getLogger("app.llm")


# --- Mod√®le de sortie attendu ---
class QuizItem(BaseModel):
    type: str = Field(description="Type de question, ici toujours 'qcm'.")
    question: str = Field(description="Intitul√© de la question.")
    choices: List[str] = Field(description="Liste des 4 choix possibles.")
    answer: str = Field(description="R√©ponse correcte.")

class QuizResponse(BaseModel):
    items: List[QuizItem]


# --- Template du prompt ---
PROMPT_TEMPLATE = """
**INSTRUCTIONS :**

Tu es un g√©n√©rateur de quiz p√©dagogique expert. Ta mission est de cr√©er un ensemble de questions √† choix multiples (QCM) bas√©es **uniquement** sur le cours fourni ci-dessous.

**R√®gle absolue :**
- N‚Äôutilise **aucune connaissance externe**.
- Chaque question et sa r√©ponse correcte doivent √™tre **directement justifiables** par le cours donn√©.

**T√ÇCHES :**
1. G√©n√®re **{nb_questions} questions QCM** pour enrichir une base de donn√©es.
2. Le jeu de questions doit **couvrir TOUT le cours** de mani√®re √©quilibr√©e : d√©finitions, concepts, noms, dates.
3. Chaque question doit :
   - √™tre courte et claire ;
   - avoir **4 choix plausibles** ;
   - contenir **une seule r√©ponse correcte** ;
   - √©viter les formulations ambigu√´s ou √©videntes.
4. Les r√©ponses doivent √™tre pr√©cises.

COURS √Ä ANALYSER :
<<<
{texte}
<<<
"""


def generate_mock_quiz(text: str, total_questions: int) -> List[dict]:
    """
    G√©n√®re un quiz mock√© pour le d√©veloppement.
    √âvite les appels API co√ªteux pendant les tests.
    """
    print(f"üß™ MODE MOCK ACTIV√â - G√©n√©ration de {total_questions} questions fictives")
    logger.info(f"Mode mock : g√©n√©ration de {total_questions} questions fictives")
    
    # Extraire quelques mots cl√©s du texte pour rendre les questions plus r√©alistes
    words = text.split()[:50]  # Premiers 50 mots
    sample_words = random.sample([w for w in words if len(w) > 4], min(10, len([w for w in words if len(w) > 4])))
    
    questions = []
    question_types = [
        "Quelle est la d√©finition de {word} ?",
        "Parmi ces propositions, laquelle concerne {word} ?",
        "Quel concept est li√© √† {word} ?",
        "Comment peut-on d√©crire {word} ?",
        "Quelle affirmation est vraie concernant {word} ?",
    ]
    
    for i in range(total_questions):
        word = sample_words[i % len(sample_words)] if sample_words else f"concept_{i+1}"
        question_template = random.choice(question_types)
        
        correct_answer = f"R√©ponse correcte sur {word}"
        wrong_answers = [
            f"Fausse r√©ponse A sur {word}",
            f"Fausse r√©ponse B sur {word}",
            f"Fausse r√©ponse C sur {word}",
        ]
        
        # M√©langer les choix
        all_choices = [correct_answer] + wrong_answers
        random.shuffle(all_choices)
        
        questions.append({
            "type": "qcm",
            "question": question_template.format(word=word),
            "choices": all_choices,
            "answer": correct_answer,
            "explanation": f"Explication mock√©e pour la question {i+1}"
        })
    
    return questions


def generate_quiz_from_text(text: str, total_questions: int) -> Tuple[List[dict], Optional[str]]:
    """
    Appelle le mod√®le Gemini pour g√©n√©rer un quiz structur√©.
    En mode MOCK, g√©n√®re des questions fictives.
    Retourne (questions, error) : questions est une liste, error est None ou un code d'erreur.
    Codes d'erreur : "quota_exceeded", "error"
    """
    if MOCK_MODE:
        return generate_mock_quiz(text, total_questions=100), None

    logger.info(f"Appel API Gemini : {total_questions} questions demand√©es")
    prompt = PROMPT_TEMPLATE.format(texte=text, nb_questions=total_questions)

    last_error = None
    for i, api_key in enumerate(API_KEYS):
        try:
            client = genai.Client(api_key=api_key)
            response = client.models.generate_content(
                model=MODEL_NAME,
                contents=prompt,
                config=types.GenerateContentConfig(
                    temperature=0.3,
                    response_mime_type="application/json",
                    response_json_schema=QuizResponse.model_json_schema()
                ),
                timeout=30  # <-- Timeout explicite pour √©viter blocage
            )

            quiz = QuizResponse.model_validate_json(response.text)
            if i > 0:
                logger.info(f"Fallback cl√© {i + 1} a fonctionn√©")
            return [item.model_dump() for item in quiz.items], None

        except Exception as e:
            # D√©tecte les erreurs de quota
            error_str = str(e).lower()
            is_quota = "resource" in error_str and "exhausted" in error_str or "429" in error_str
            last_error = e
            key_label = f"cl√© {i + 1}"

            if is_quota and i < len(API_KEYS) - 1:
                logger.warning(f"Quota d√©pass√© ({key_label}), bascule sur la cl√© suivante")
                continue
            elif is_quota:
                logger.error(f"Quota d√©pass√© sur toutes les cl√©s")
                return [], "quota_exceeded"
            else:
                logger.error(f"Erreur API ou r√©seau ({key_label}) : {e}")
                return [], "error"

    logger.error(f"Aucune cl√© API disponible ou toutes les requ√™tes ont √©chou√© : {last_error}")
    return [], "error"
